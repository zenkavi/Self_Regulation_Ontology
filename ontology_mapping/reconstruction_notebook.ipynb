{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Reconstructing out-of-sample DVs\n",
    "\n",
    "Given a quantitative ontology, or psychological space, that DVs can be projected into, how can we deterine the embedding of new variables?\n",
    "\n",
    "Currently, our embedding is determined by factor analysis. Thus ontological embedding are only known for the DVs entered into the original model. How could we extend this?\n",
    "\n",
    "One possibility is measuring new variables in the same population that completed our original battery. After doing this we could either (1) run the model anew, or (2) use linear regression to map the already discovered factors onto the new variables. The former is better, but results in small changes to the actual factors with each new variable. The latter method ensures that our factors stay the same. Neither is scalable, however, as we do not, in general, have access to a constant population that can be remeasured whenever new measures come into the picture.\n",
    "\n",
    "Another possibility that works with new populations requires that the new population completes the entire battery used to estimate the original factors, in addition to whatever new variables are of interest. Doing so allows the calculation of factor scores for this new population based on the original model, which can then be mapped to the new measures of interest. This allows researchers to capitalize on the original model (presumably fit on more subjects than the new study), while expanding the ontology. Problems exist here, however.\n",
    "- The most obvious problem is that you have to measure the new sample on the entire battery used to fit the original EFA model. Given that this takes many hours (the exact number depending on whether tasks, surveys or both are used), this is exceedingly impractical. In our cas we did have our new fMRI sample take the entire battery (or at least a subset of participants), so this problem isn't as relevant\n",
    "- Still problems remain. If N is small, the estimate of the ontological embedding for new DVs is likely unstable.\n",
    "\n",
    "This latter problem necessitates some quantitative exploration. This notebook simulates the issue by:\n",
    "1. Removing a DV from the original ontology dataset\n",
    "2. Performing EFA on this subset\n",
    "3. Using linear regression to map these EFA factors to the left out variable\n",
    "\n",
    "(3) is performed on smaller population sizes to reflect the reality of most studies (including ours) and is repeated to get a sense of the mapping's variability\n",
    "\n",
    "### Small issues not currently addressed\n",
    "\n",
    "- The EFA model is fit on the entire population. An even more stringent simulation would subset the subjects used in the \"new study\" and fit the EFA model on a completely independent group. I tried this once - the factor scores hardly differed. In addition, I want the EFA model to be as well-powered as possible, as that will be the reality for this method moving forward\n",
    "- I am currently not holding out entire tasks, but only specific DVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from os import makedirs, path\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from dimensional_structure.EFA_plots import get_communality\n",
    "from dimensional_structure.utils import abs_pdist\n",
    "from ontology_mapping.reconstruction_plots import (plot_factor_reconstructions,\n",
    "                                                       plot_reconstruction_hist)\n",
    "from ontology_mapping.reconstruction_utils import (get_reconstruction_results, \n",
    "                                                   linear_reconstruction,\n",
    "                                                   k_nearest_reconstruction,\n",
    "                                                  CV_predict)\n",
    "from selfregulation.utils.plot_utils import format_num, save_figure\n",
    "from selfregulation.utils.result_utils import load_results\n",
    "from selfregulation.utils.utils import get_recent_dataset, get_info, get_retest_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore some warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=sklearn.metrics.classification.UndefinedMetricWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# argparse\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-pop_sizes', nargs='+', default=[25, 50, 100, 400], type=int)\n",
    "    parser.add_argument('-n_reps', default=200)\n",
    "    parser.add_argument('-n_measures', default=None, type=int)\n",
    "    parser.add_argument('-dataset', default=None)\n",
    "    parser.add_argument('-rerun', action='store_true')\n",
    "    parser.add_argument('-append', action='store_true')\n",
    "    parser.add_argument('-EFA_rotation', default='oblimin')\n",
    "    parser.add_argument('-knn_metric', default='correlation')\n",
    "    parser.add_argument('-verbose', action='store_true')\n",
    "    parser.add_argument('-save', action='store_true')\n",
    "    args, _ = parser.parse_known_args()\n",
    "    pop_sizes = args.pop_sizes\n",
    "    n_reps = args.n_reps\n",
    "    n_measures = args.n_measures\n",
    "    rerun = args.rerun\n",
    "    append = args.append\n",
    "    knn_metric = args.knn_metric\n",
    "    EFA_rotation = args.EFA_rotation\n",
    "    verbose = args.verbose\n",
    "    save = args.save\n",
    "    if args.dataset is not None:\n",
    "        dataset = args.dataset\n",
    "    else:\n",
    "        dataset = get_recent_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional setup\n",
    "np.random.seed(12412)\n",
    "results = load_results(dataset)['task']\n",
    "retest_data = get_retest_data(dataset.replace('Complete', 'Retest'))\n",
    "c = results.EFA.results['num_factors']\n",
    "\n",
    "classifiers = {'Ridge': Ridge(fit_intercept=False),\n",
    "               'LR': LinearRegression(fit_intercept=False)}\n",
    "# get output dir to store results\n",
    "output_dir = path.join(get_info('results_directory'),\n",
    "                       'ontology_reconstruction', results.ID, EFA_rotation)\n",
    "makedirs(output_dir, exist_ok=True)\n",
    "# get plot dir to store plots\n",
    "plot_dir = path.join(output_dir, 'Plots')\n",
    "makedirs(plot_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a random subset of variables to perform the calculation on if n_vars is set\n",
    "measures = np.unique([i.split('.')[0] for i in results.data.columns])\n",
    "if n_measures is not None:\n",
    "    measure_list = np.random.choice(measures, n_measures, replace=False)\n",
    "else:\n",
    "    measure_list = measures\n",
    "# get all variables from selected tasks\n",
    "var_list = results.data.filter(regex='|'.join(measure_list)).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(reconstruction_files, query_string=None):\n",
    "    out = {}\n",
    "    for f in reconstruction_files:\n",
    "        tmp = pd.read_pickle(f)\n",
    "        if query_string:\n",
    "            tmp = tmp.query(query_string)\n",
    "        name = f.split('-')[-1][:-4]\n",
    "        out[name] = tmp\n",
    "    return out\n",
    "\n",
    "def update_files(old, new):\n",
    "    for k, df in old.items():\n",
    "        add = new.pop(k)\n",
    "        add = add.query('label == \"partial_reconstruct\"')\n",
    "        add.loc[:,'rep'] += df.rep.max()\n",
    "        old[k] = pd.concat([df, add], sort=False).reset_index(drop=True)\n",
    "    old.update(new)\n",
    "        \n",
    "def combine_files(reconstruction_files, query_string=None):\n",
    "    if type(reconstruction_files) != dict:\n",
    "        out = load_files(reconstruction_files, query_string)\n",
    "    else:\n",
    "        out = reconstruction_files\n",
    "    return pd.concat(out, sort=False).reset_index(drop=True)\n",
    "\n",
    "def normalize_reconstruction(reconstruction, c, inplace=True):\n",
    "    \"\"\" Ensures reconstructions lie on the unit circle \"\"\"\n",
    "    if not inplace:\n",
    "        reconstruction = reconstruction.copy()\n",
    "    normed = normalize(reconstruction.iloc[:,:c])\n",
    "    reconstruction.iloc[:,:c] = normed\n",
    "    if not inplace:\n",
    "        return reconstruction\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run simulation for every variable at different population sizes. \n",
    "\n",
    "That is, do the following:\n",
    "\n",
    "1. take a variable (say stroop incongruent-congruent RT), remove it from the data matrix\n",
    "2. Run EFA on the data matrix composes of the 522 (subject) x N-1 (variable) data matrix\n",
    "3. Calculate factor scores for all 522 subjects\n",
    "4. Select a subset of \"pop_size\" to do an \"ontological mapping\". That is, pretend that these subjects did the whole battery (missing the one variable) *and then* completed one more task. The idea is we want to do a mapping from those subject's factor scores to the new variable\n",
    "   1. We can do a linear mapping (regression) from the ontological scores to the output variable\n",
    "   2. We can do a k-nearest neighbor interpolation, where we say the unknown ontological factor is a blend of the \"nearest\" variables in the dataset\n",
    "5. Repeat (4) a number of times to get a sense for the accuracy and variability of that mapping\n",
    "6. Compare the estimated ontological scores for the held out var (stroop incongruent-congruent) to the original \"correct\" ontological mapping (that would have been obtained if the variable was included in the original data matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbor Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "k_list = list(range(1,20))\n",
    "basename = path.join(output_dir, 'KNNR_%s-*' % knn_metric)\n",
    "files = glob(basename)\n",
    "updated = []\n",
    "if rerun: # rerun everything\n",
    "    regex_list = ['^'+m for m in measure_list]\n",
    "    k_reconstructions=get_reconstruction_results(results, regex_list, pop_sizes, \n",
    "                                                 n_reps=n_reps, \n",
    "                                                 recon_fun=k_nearest_reconstruction, \n",
    "                                                 k_list=k_list, \n",
    "                                                 metric=knn_metric,\n",
    "                                                 EFA_rotation=EFA_rotation,\n",
    "                                                 verbose=verbose)\n",
    "    updated = list(k_reconstructions)\n",
    "else:\n",
    "    k_reconstructions = load_files(files)\n",
    "    if append: # add more simulations to previous files\n",
    "        regex_list = ['^'+m for m in measure_list]\n",
    "        to_append = get_reconstruction_results(results, regex_list, pop_sizes, \n",
    "                                                n_reps=n_reps, \n",
    "                                                recon_fun=k_nearest_reconstruction, \n",
    "                                                k_list=k_list, \n",
    "                                                metric=knn_metric,\n",
    "                                                EFA_rotation=EFA_rotation,\n",
    "                                               verbose=verbose)\n",
    "        updated = list(to_append.keys())\n",
    "        update_files(k_reconstructions, to_append)\n",
    "    else: # load previous files and add run any additional ones required\n",
    "        tmp_measures = set(measure_list) - set(k_reconstructions.keys())\n",
    "        regex_list = ['^'+m for m in tmp_measures]\n",
    "        additional = get_reconstruction_results(results, regex_list, pop_sizes, \n",
    "                                                n_reps=n_reps, \n",
    "                                                recon_fun=k_nearest_reconstruction, \n",
    "                                                k_list=k_list, \n",
    "                                                metric=knn_metric,\n",
    "                                                EFA_rotation=EFA_rotation,\n",
    "                                                verbose=verbose)\n",
    "        k_reconstructions.update(additional)\n",
    "        updated = additional.keys()\n",
    "\n",
    "for measure in updated:\n",
    "    df = k_reconstructions[measure]\n",
    "    if save:\n",
    "        df.to_pickle(basename[:-1]+'%s.pkl' % measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "k_list = list(range(1,20))\n",
    "basename = path.join(output_dir, 'KNNRind_%s-*' % knn_metric)\n",
    "files = glob(basename)\n",
    "updated = []\n",
    "pop_size_subset = [i for i in pop_sizes if i < 100]\n",
    "if rerun: # rerun everything\n",
    "    regex_list = ['^'+m for m in measure_list]\n",
    "    k_reconstructions_ind=get_reconstruction_results(results, regex_list, pop_size_subset, \n",
    "                                                 n_reps=n_reps, \n",
    "                                                 recon_fun=k_nearest_reconstruction, \n",
    "                                                 k_list=k_list, \n",
    "                                                 metric=knn_metric,\n",
    "                                                 EFA_rotation=EFA_rotation,\n",
    "                                                 independent_EFA=True,\n",
    "                                                 verbose=verbose)\n",
    "    updated = measure_list\n",
    "else:\n",
    "    k_reconstructions_ind = load_files(files)\n",
    "    if append: # add more simulations to previous files\n",
    "        regex_list = ['^'+m for m in measure_list]\n",
    "        to_append = get_reconstruction_results(results, regex_list, pop_size_subset, \n",
    "                                                n_reps=n_reps, \n",
    "                                                recon_fun=k_nearest_reconstruction, \n",
    "                                                k_list=k_list, \n",
    "                                                metric=knn_metric,\n",
    "                                                EFA_rotation=EFA_rotation,\n",
    "                                                independent_EFA=True,\n",
    "                                                verbose=verbose)\n",
    "        updated = list(to_append.keys())\n",
    "        update_files(k_reconstructions_ind, to_append)\n",
    "    else: # load previous files and add run any additional ones required\n",
    "        tmp_measures = set(measure_list) - set(k_reconstructions_ind.keys())\n",
    "        regex_list = ['^'+m for m in tmp_measures]\n",
    "        additional = get_reconstruction_results(results, regex_list, pop_size_subset, \n",
    "                                                n_reps=n_reps, \n",
    "                                                recon_fun=k_nearest_reconstruction, \n",
    "                                                k_list=k_list, \n",
    "                                                metric=knn_metric,\n",
    "                                                EFA_rotation=EFA_rotation,\n",
    "                                                independent_EFA=True,\n",
    "                                                verbose=verbose)\n",
    "        k_reconstructions_ind.update(additional)\n",
    "        updated = additional.keys()\n",
    "\n",
    "for measure in updated:\n",
    "    df = k_reconstructions_ind[measure]\n",
    "    if save:\n",
    "        df.to_pickle(basename[:-1]+'-%s.pkl' % measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_k(k_reconstructions):\n",
    "    var_summary = pd.DataFrame()\n",
    "    for measure, reconstruction in k_reconstructions.items():\n",
    "        tmp_summary = reconstruction.query('label==\"partial_reconstruct\"') \\\n",
    "                        .groupby(['pop_size', 'k', 'weighting','var'])['corr_score'].mean().reset_index()\n",
    "        var_summary = pd.concat([var_summary, tmp_summary])\n",
    "    \n",
    "    k_summary = var_summary.groupby(['pop_size', 'k', 'weighting']).mean()\n",
    "    # summarize further\n",
    "    k_best_params = {}\n",
    "    for pop_size in k_summary.index.unique(level='pop_size'):\n",
    "        tmp=k_summary.query('pop_size == %s' % pop_size)\n",
    "        best_params = tmp.idxmax()[0]\n",
    "        best_val = tmp.loc[best_params][0]\n",
    "        k_best_params[pop_size] = {'k': best_params[1], \n",
    "                                   'weighting': best_params[2],\n",
    "                                   'best_val': best_val}\n",
    "    reconstruction_list = []\n",
    "    for reconstruction in k_reconstructions.values():\n",
    "        true = reconstruction.query('label == \"true\"')\n",
    "        reconstruction_list.append(true)\n",
    "        for k, v in k_best_params.items():\n",
    "            tmp_partial = reconstruction.query('pop_size == %s and \\\n",
    "                                         k == %s and \\\n",
    "                                         weighting == \"%s\"' % (k, v['k'], v['weighting']))\n",
    "            full = reconstruction.query('label == \"full_reconstruct\" and \\\n",
    "                                         k == %s and \\\n",
    "                                         weighting == \"%s\"' % (v['k'], v['weighting']))\n",
    "            reconstruction_list += [tmp_partial, full]\n",
    "\n",
    "    k_best_reconstruction = pd.concat(reconstruction_list, axis=0, sort=False)\n",
    "    return var_summary, k_best_params, k_best_reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_var_summary, k_best_params, k_reconstruction = summarize_k(k_reconstructions)\n",
    "k_reconstruction.query('label==\"partial_reconstruct\"') \\\n",
    "    .groupby('pop_size')['corr_score'].agg(['mean','std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_ind_var_summary, k_best_params, k_ind_reconstruction = summarize_k(k_reconstructions_ind)\n",
    "k_ind_reconstruction.query('label==\"partial_reconstruct\"') \\\n",
    "    .groupby('pop_size')['corr_score'].agg(['mean','std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del k_reconstructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "clfs = {'Linear': LinearRegression(fit_intercept=False),\n",
    "       'RidgeCV': RidgeCV(fit_intercept=False, cv=10)}\n",
    "linear_reconstructions = {}\n",
    "for clf_name, clf in clfs.items():\n",
    "    basename = path.join(output_dir, 'linear-%s_reconstruct*' % clf_name)\n",
    "    files = glob(basename)\n",
    "    updated = []\n",
    "    if rerun: # rerun everything\n",
    "        regex_list = ['^'+m for m in measure_list]\n",
    "        tmp_reconstructions=get_reconstruction_results(results, regex_list, pop_sizes, \n",
    "                                                       n_reps=n_reps, \n",
    "                                                       recon_fun=linear_reconstruction, \n",
    "                                                       clf=clf,\n",
    "                                                       EFA_rotation=EFA_rotation,\n",
    "                                                       verbose=verbose)\n",
    "        updated = measure_list\n",
    "    else:\n",
    "        tmp_reconstructions = load_files(files)\n",
    "        if append: # add more simulations to previous files\n",
    "            regex_list = ['^'+m for m in measure_list]\n",
    "            to_append = get_reconstruction_results(results, regex_list, pop_sizes, \n",
    "                                                       n_reps=n_reps, \n",
    "                                                       recon_fun=linear_reconstruction, \n",
    "                                                       clf=clf,\n",
    "                                                       EFA_rotation=EFA_rotation,\n",
    "                                                       verbose=verbose)\n",
    "            updated = list(to_append.keys())\n",
    "            update_files(tmp_reconstructions, to_append)\n",
    "        else: # load previous files and add run any additional ones required\n",
    "            tmp_measures = set(measure_list) - set(tmp_reconstructions.keys())\n",
    "            regex_list = ['^'+m for m in tmp_measures]\n",
    "            additional = get_reconstruction_results(results, regex_list, pop_sizes, \n",
    "                                                    n_reps=n_reps, \n",
    "                                                    recon_fun=linear_reconstruction, \n",
    "                                                    clf=clf,\n",
    "                                                    EFA_rotation=EFA_rotation,\n",
    "                                                    verbose=verbose)\n",
    "            tmp_reconstructions.update(additional)\n",
    "            updated = additional.keys()\n",
    "\n",
    "    for measure in updated:\n",
    "        df = tmp_reconstructions[measure]\n",
    "        if save:\n",
    "            df.to_pickle(basename[:-1]+'-%s.pkl' % measure)\n",
    "        \n",
    "    files = glob(basename)\n",
    "    if len(files) > 0:\n",
    "        linear_reconstructions[clf_name] = combine_files(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame()\n",
    "for clf, df in linear_reconstructions.items():\n",
    "    tmp = df.query('label==\"partial_reconstruct\"') \\\n",
    "        .groupby('pop_size').corr_score.agg([np.mean, np.std])\n",
    "    tmp.loc[:,'clf'] = clf\n",
    "    summary = pd.concat([summary, tmp], sort=False)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructions = {'KNN': k_reconstruction,\n",
    "                   'RidgeCV': linear_reconstructions['RidgeCV']}\n",
    "reconstructed_vars = sorted(k_reconstruction['var'].unique())\n",
    "assert set(reconstructed_vars) == set(reconstructions['RidgeCV']['var'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable characteristics that influence reconstruction quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable characteristics\n",
    "reconstruction = reconstructions['KNN']\n",
    "retest_index = [i.replace('.logTr','').replace('.ReflogTr','') for i in reconstructed_vars]\n",
    "retest_vals = retest_data.loc[retest_index,'icc']\n",
    "retest_vals.index = reconstructed_vars\n",
    "communality = get_communality(results.EFA).loc[reconstructed_vars]\n",
    "avg_corr  = abs(results.data.corr()).replace(1,0).mean()\n",
    "avg_corr.name = \"avg_correlation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create summaries\n",
    "additional = pd.concat([retest_vals, communality, avg_corr], axis=1)\n",
    "reconstruction_summaries = {}\n",
    "for name, reconstruction in reconstructions.items():\n",
    "    s = reconstruction.query('label == \"partial_reconstruct\"') \\\n",
    "        .groupby(['var', 'pop_size']).corr_score.agg(['mean', 'std'])\n",
    "    s = s.reset_index().join(additional, on='var')\n",
    "    reconstruction_summaries[name] = s\n",
    "all_reconstructions = pd.concat(reconstruction_summaries).reset_index()\n",
    "all_reconstructions = all_reconstructions.rename({'level_0': 'approach'}, axis=1).drop('level_1', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does reconstruction success at one population size predict the next?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for i,group in all_reconstructions.groupby(['approach', 'pop_size']):\n",
    "    group = group.loc[:,['var','mean']].set_index('var')\n",
    "    group.columns = [i]\n",
    "    tmp.append(group)\n",
    "approach_compare = pd.concat(tmp, axis=1)\n",
    "approach_compare.columns = [i+': '+str(int(j)) for i,j in approach_compare.columns]\n",
    "# correlation of reconstructions\n",
    "corr= approach_compare.corr(method='spearman')\n",
    "overall_correlation = np.mean(corr.values[np.tril_indices_from(corr, -1)])\n",
    "print('DV reconstruction score correlates %s across approaches' % format_num(overall_correlation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model reconstruction success as a function of DV characteristics, approach and subpopulation size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patsy.contrasts import ContrastMatrix\n",
    "\n",
    "def _name_levels(prefix, levels):\n",
    "    return [\"[%s%s]\" % (prefix, level) for level in levels]\n",
    "\n",
    "class Simple(object):\n",
    "    def _simple_contrast(self, levels):\n",
    "        nlevels = len(levels)\n",
    "        contr = -1./nlevels * np.ones((nlevels, nlevels-1))\n",
    "        contr[1:][np.diag_indices(nlevels-1)] = (nlevels-1.)/nlevels\n",
    "        return contr\n",
    "\n",
    "    def code_with_intercept(self, levels):\n",
    "        contrast = np.column_stack((np.ones(len(levels)),\n",
    "                                    self._simple_contrast(levels)))\n",
    "        return ContrastMatrix(contrast, _name_levels(\"Simp.\", levels))\n",
    "\n",
    "    def code_without_intercept(self, levels):\n",
    "        contrast = self._simple_contrast(levels)\n",
    "        return ContrastMatrix(contrast, _name_levels(\"Simp.\", levels[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mdf = md.fit()\n",
    "mdf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast = Simple().code_without_intercept([0,1])\n",
    "print(contrast.matrix)\n",
    "all_reconstructions.loc[:, 'z_mean'] = np.arctanh(all_reconstructions['mean'])\n",
    "md = smf.mixedlm(\"z_mean ~ (pop_size + icc + avg_correlation)*C(approach, Sum)\", all_reconstructions, groups=all_reconstructions[\"var\"])\n",
    "mdf = md.fit()\n",
    "mdf.summary()\n",
    "\n",
    "# other way to do it\n",
    "# endog, exog = patsy.dmatrices(\"z_mean ~ (pop_size + icc + avg_correlation)*C(approach, Sum)\", all_reconstructions, return_type='dataframe')\n",
    "# md = sm.MixedLM(endog=endog, exog=exog, groups=all_reconstructions['var'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How well are we reconstructing distances?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_loadings = results.EFA.get_loading(c=c).loc[reconstructed_vars]\n",
    "orig_distances = pd.DataFrame(squareform(abs_pdist(orig_loadings)), index=orig_loadings.index, columns=orig_loadings.index)\n",
    "\n",
    "reconstructed_distances = {}\n",
    "for name, reconstruction in reconstructions.items():\n",
    "    for pop_size in sorted(reconstruction.pop_size.dropna().unique()):\n",
    "        reconstructed_loadings = reconstruction.query('pop_size == %s' % pop_size).groupby('var').mean().iloc[:,:c]\n",
    "        distances = pd.DataFrame(squareform(abs_pdist(reconstructed_loadings)), \n",
    "                                index=orig_loadings.index, columns=orig_loadings.index)\n",
    "        reconstructed_distances[name+'_%03d' % pop_size] = distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How well can we recover the variable from the embedding? How differentiable are the embeddings?\n",
    "\n",
    "The recovery of distances seems like the better analysis, but keeping this in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify each variable/cluster based on its embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = results.HCA.results['EFA5_oblimin']['distance_df']\n",
    "clusters = results.HCA.results['EFA5_oblimin']['labels']\n",
    "cluster_map = {l:c for l,c in zip(labels,clusters)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classification_scores = {}\n",
    "for key, reconstruction in reconstructions.items():\n",
    "    # save scores\n",
    "    var_scores = {}\n",
    "    cluster_scores = {}\n",
    "    # set up input\n",
    "    true = reconstruction.query('label == \"true\"')\n",
    "    test_set = [true.iloc[:,:c].values]\n",
    "    var_test_set = test_set + [true['var']]\n",
    "    cluster_test_set = test_set + [true['var'].apply(lambda x: cluster_map[x])]\n",
    "    clf=KNeighborsClassifier(weights='distance', n_neighbors=10)\n",
    "    \n",
    "    for pop_size in [i for i in reconstruction.pop_size.unique() if str(i) != 'nan']:\n",
    "        reconstruction_subset = reconstruction.query('pop_size==%s and label==\"partial_reconstruct\"' % pop_size)\n",
    "        var_scores[pop_size] = CV_predict(reconstruction_subset, reconstruction_subset['var'],\n",
    "                                          clf=clf, cv=10, test_set=var_test_set)\n",
    "        # classify based on clusters\n",
    "        cluster_labels = reconstruction_subset['var'].apply(lambda x: cluster_map[x])\n",
    "        cluster_scores[pop_size] = CV_predict(reconstruction_subset, cluster_labels,\n",
    "                                      clf=clf, cv=10, test_set=cluster_test_set)\n",
    "    classification_scores[key] = {'var_scores': var_scores,\n",
    "                                  'cluster_scores': cluster_scores}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When the true values are misclassified, what are they classified as?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only calculating this for one method as I won't be using this analysis.\n",
    "tmp_classification = classification_scores['RidgeCV']\n",
    "tmp_reconstruction = reconstructions['RidgeCV']\n",
    "var_scores = tmp_classification['var_scores'] \n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(reconstruction['var'])\n",
    "confusion = var_scores[400]['true_confusion']\n",
    "classification = {}\n",
    "for i, confusion_vector in enumerate(confusion):\n",
    "    true_label = le.inverse_transform([i])[0]\n",
    "    classified_label = le.inverse_transform([np.where(confusion_vector)[0][0]])[0]\n",
    "    classification[true_label] = classified_label\n",
    "    if verbose and (true_label!=classified_label):\n",
    "        print('True: %s\\nClassified: %s' % (true_label, classified_label))\n",
    "        print('*'*79)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How much gaussian noise equates the performance of this classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "reps = 100\n",
    "clf=KNeighborsClassifier(weights='distance', n_neighbors=10)\n",
    "noisy_scores = {}\n",
    "# hyperparameters\n",
    "for noise_val in np.linspace(.05, .5, 20):\n",
    "    true = tmp_reconstruction.query('label == \"true\"')\n",
    "    labels = true['var'].tolist()*reps\n",
    "    embeddings = true.iloc[:,:c].values\n",
    "    noise = np.random.normal(0, (embeddings.std()*noise_val), size=[reps,c])\n",
    "    simulated_noise = np.empty([0,c])\n",
    "    for n in noise:\n",
    "        simulated_noise = np.vstack([simulated_noise, embeddings+n])\n",
    "    noisy_data = pd.DataFrame(simulated_noise, columns=true.columns[:c])\n",
    "    noisy_data.loc[:,'var'] = labels\n",
    "    noisy_scores[noise_val] = CV_predict(noisy_data, noisy_data['var'],\n",
    "                                           clf=clf, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Of concern is the average correspondence and variability between the estimated ontological fingerprint of a DV and its \"ground-truth\" (the original estimate when it was part of the EFA model)\n",
    "\n",
    "One way to look at this is just the average reconstruction score (e.g., for example) and variability of reconstruction score as a function of pseudo-pop-size and model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = sns.color_palette('Set1', n_colors = len(pop_sizes), desat=.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(12,8))\n",
    "sns.boxplot(x='pop_size', y='mean', hue='approach', data=all_reconstructions, palette='Reds')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot relationship of performance for each DV over different approach parameterizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = approach_compare.corr(method='spearman')\n",
    "mean_success = approach_compare.mean()\n",
    "plot_df = approach_compare.join(retest_vals).join(avg_corr)\n",
    "size = 2\n",
    "f=sns.pairplot(plot_df.iloc[:,0:8], height=size,\n",
    "             plot_kws={'color': [.4,.4,.4],\n",
    "                       's': plot_df['avg_correlation']*1000},\n",
    "             diag_kws={'bins': 20,\n",
    "                      'edgecolor': 'k',\n",
    "                      'linewidth': size/4})\n",
    "axes = f.axes\n",
    "# fix axes limits\n",
    "for i in range(len(f.axes)):\n",
    "    for j in range(len(f.axes)):\n",
    "        ax = axes[i][j]\n",
    "        ax.set_ylim([.2,1.1])\n",
    "        ax.tick_params(left=False, bottom=False,\n",
    "                      labelleft=False, labelbottom=False)\n",
    "        if i!=j:\n",
    "            ax.set_xlim([.2,1.1])\n",
    "            ax.plot(ax.get_xlim(), ax.get_ylim(), lw=size, ls=\"--\", c=\".3\", zorder=-1)\n",
    "        if j<i:\n",
    "            x = .6; y = .3\n",
    "            if mean_success[j] > mean_success[i]:\n",
    "                x = .28; y = 1\n",
    "            ax.text(x, y, r'$\\rho$ = %s' % format_num(corr.iloc[i,j]),\n",
    "                   fontsize=size*8)\n",
    "        # change sizing for upper triangle based on icc\n",
    "        if j>i: \n",
    "            ax.set_visible(False)\n",
    "            #ax.collections[0].set_sizes(plot_df['icc']**2*100)\n",
    "            \n",
    "# color diagonal\n",
    "for i,ax in enumerate(f.diag_axes):\n",
    "    ax.set_title(axes[i][0].get_ylabel(), color=colors[i%4], fontsize=size*10)\n",
    "    for patch in ax.patches:\n",
    "        patch.set_facecolor(colors[i%4])\n",
    "        \n",
    "# color labels\n",
    "for i in range(len(f.axes)):\n",
    "    left_ax = axes[i][0]\n",
    "    bottom_ax = axes[-1][i]\n",
    "    left_ax.set_ylabel(left_ax.get_ylabel(), color=colors[i%4],labelpad=10, fontsize=size*10)\n",
    "    bottom_ax.set_xlabel('')\n",
    "    \n",
    "# set tick spacing\n",
    "ax = axes[-1][-2]\n",
    "ax.tick_params(length=1, width=1, labelleft=True, labelbottom=True)\n",
    "ax.set_xticks([.18, 1])\n",
    "ax.set_xticklabels(['0.2', '1.0'], fontsize=size*8, fontweight='bold')\n",
    "ax.set_yticks([1])\n",
    "ax.set_yticklabels(['1.0'], fontsize=size*8, fontweight='bold')\n",
    "# common X\n",
    "f.fig.text(0.5, 0.02, 'Average DV Reconstruction Score', ha='center', fontsize=size*10)\n",
    "if save:\n",
    "    save_figure(f, path.join(plot_dir, 'SFig1_cross_approach_correlations.png'), save_kws={'dpi': 300})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "### K Nearest Visualization (Example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average Performance by Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = k_var_summary.reset_index()\n",
    "sns.set_context('talk')\n",
    "n_cols = 1\n",
    "n_rows = len(pop_sizes)//n_cols\n",
    "f, axes = plt.subplots(n_rows, n_cols, figsize=(12,n_rows*6))\n",
    "axes = f.get_axes()\n",
    "legend_on=True\n",
    "for ax, pop_size in zip(axes, pop_sizes):\n",
    "    sns.pointplot(x='k', y='corr_score', hue='weighting', \n",
    "                data=plot_df.query('pop_size==%s' % pop_size),\n",
    "                ax=ax, dodge=.3, alpha=.75, join=False, ci=None)\n",
    "    ax.set_title('Simulated Population Size: %s' % pop_size)\n",
    "    ax.set_ylim(-1.1,1.1)\n",
    "    ax.legend().set_visible(legend_on)\n",
    "    legend_on=False\n",
    "plt.subplots_adjust(hspace=.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "#### Performance for each DV\n",
    "\n",
    "Only taking the best parameters from the k-nearest neighbor algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = \"simon.hddm_drift\"\n",
    "ax = reconstructions['KNN'].query('var == \"%s\" and pop_size==100' % var).corr_score.hist(bins=30,\n",
    "                                                                          edgecolor='white',\n",
    "                                                                           figsize=[10,6])\n",
    "ax.set_xlabel('Reconstruction Score', fontsize=40, labelpad=30)\n",
    "ax.set_yticklabels([])\n",
    "ax.set_yticks([])\n",
    "ax.xaxis.set_major_locator(ticker.MultipleLocator(.05))\n",
    "ax.tick_params(labelsize=30)\n",
    "ax.spines['left'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Histogram of DV reconstruction scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reconstruction_hist(reconstructions['KNN'], title='KNN Reconstruction', size=14)\n",
    "plot_reconstruction_hist(reconstructions['RidgeCV'], title='RidgeCV Reconstruction', size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving\n",
    "if save:\n",
    "    plot_reconstruction_hist(reconstructions['KNN'], title='KNN Reconstruction', size=14,\n",
    "                            filename=path.join(plot_dir, 'KNN_reconstruction.png'))\n",
    "    plot_reconstruction_hist(reconstructions['RidgeCV'], title='RidgeCV Reconstruction', size=14,\n",
    "                            filename=path.join(plot_dir, 'RidgeCV_reconstruction.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is clearly a bit of variability in the reconstruction accuracy based on the variable itself. While this variability narrows with larger populations, it's still there, and there are a few variables that cannot be reconstructed at all\n",
    "\n",
    "One possibility is that the least reliable variables are the worst reconstructed. Let's look at that..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reconstruction score vs. Reliability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "f, axes = plt.subplots(1,2,figsize=(14,6))\n",
    "for i, pop_size in enumerate(pop_sizes):\n",
    "    sns.regplot('icc', 'mean', \n",
    "                data=reconstruction_summaries['KNN'].query('pop_size==%s' % pop_size), \n",
    "                label=pop_size, ax=axes[0], color=colors[i])\n",
    "    sns.regplot('icc', 'std', \n",
    "                data=reconstruction_summaries['KNN'].query('pop_size==%s' % pop_size), \n",
    "                label=pop_size, ax=axes[1], color=colors[i])\n",
    "axes[1].legend()\n",
    "plt.suptitle('Reliability vs Reconstruction Score Mean/Std')\n",
    "plt.subplots_adjust(wspace=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can dive in and look at one high/mediun/low reliable variable to see the reconstruction performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_retest_vals = retest_vals.sort_values().index\n",
    "N = len(sorted_retest_vals)\n",
    "high_var = sorted_retest_vals[N-1]\n",
    "med_var = sorted_retest_vals[N//2]\n",
    "low_var = sorted_retest_vals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(1,3, figsize=(20,8))\n",
    "for ax, var in zip(axes, [high_var, med_var, low_var]):\n",
    "    retest_in = var.replace('.logTr','').replace('.ReflogTr','')\n",
    "    reliability = format_num(retest_data.loc[retest_in]['icc'])\n",
    "    plot_df = k_reconstruction.query('var == \"%s\" and label==\"partial_reconstruct\"' % var)\n",
    "    sns.boxplot(x='pop_size', y='corr_score', data=plot_df,  ax=ax)\n",
    "    ax.set_title('%s\\nICC: %s' % (var, reliability))\n",
    "plt.subplots_adjust(wspace=.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that doesn't seem to be the whole story. Instead, what might be important is the DVs actual relationship to the ontology. That is, some DVs are better captured by the ontology to begin with. Maybe DVs \"peripheral\" to this particular ontology (not well captured by the space) are poorly reconstructed.\n",
    "\n",
    "We can look at this by looking at the relationship between communality and DV reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reconstruction Score vs Communality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "f, axes = plt.subplots(1,2,figsize=(14,6))\n",
    "for i, pop_size in enumerate(pop_sizes):\n",
    "    sns.regplot('communality', 'mean', \n",
    "                data=reconstruction_summaries['KNN'].query('pop_size==%s' % pop_size), \n",
    "                label=pop_size, ax=axes[0], color=colors[i])\n",
    "    sns.regplot('communality', 'std', \n",
    "                data=reconstruction_summaries['KNN'].query('pop_size==%s' % pop_size), \n",
    "                label=pop_size, ax=axes[1], color=colors[i])\n",
    "axes[1].legend()\n",
    "plt.suptitle('Communality vs Reconstruction Score Mean/Std')\n",
    "plt.subplots_adjust(wspace=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems clear that DVs with poor communality are not reconstructed well. A less \"analysis based\" way to think about this is reconstruction will be worse if you are far away from the other variables in the set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reconstruction Score vs Average Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk')\n",
    "f, axes = plt.subplots(1,2,figsize=(14,6))\n",
    "for i, pop_size in enumerate(pop_sizes):\n",
    "    sns.regplot('avg_correlation', 'mean', \n",
    "                data=reconstruction_summaries['KNN'].query('pop_size==%s' % pop_size), \n",
    "                label=pop_size, ax=axes[0], color=colors[i])\n",
    "    sns.regplot('avg_correlation', 'std', \n",
    "                data=reconstruction_summaries['KNN'].query('pop_size==%s' % pop_size), \n",
    "                label=pop_size, ax=axes[1], color=colors[i])\n",
    "axes[1].legend()\n",
    "plt.suptitle('Average Correlation vs Reconstruction Score Mean/Std')\n",
    "plt.subplots_adjust(wspace=.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the correlation with the overall dataset is important for reconstruction. All of this says that ontological mapping will be more successful if you have an a-priori reason to believe your new variable has something to do with the rest of the variables in the ontology. The weaker you believe that bond, the more data you should collect to articulate the connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of classification success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_CV_predict(scores, pop_sizes, size):\n",
    "    f, axes = plt.subplots(len(pop_sizes), 2, figsize=(size*2, len(pop_sizes)*size))\n",
    "    if len(axes.shape) == 1:\n",
    "        axes = [axes]\n",
    "    for ax, pop_size in zip(axes, pop_sizes):\n",
    "        precision, recall, f1, confusion = [scores[pop_size][i] for i in ['precision',\n",
    "                                                                          'recall',\n",
    "                                                                          'f1',\n",
    "                                                                          'confusion']]\n",
    "        # plot cross validated scores\n",
    "        sns.heatmap(confusion, ax=ax[0], square=True, xticklabels=False, yticklabels=False, cbar=False)\n",
    "        ax[0].set_title('Pop_size: %s, Recall: %s, Precision: %s, F1: %s' % (format_num(pop_size),\n",
    "                                                                          format_num(precision), \n",
    "                                                                          format_num(recall), \n",
    "                                                                          format_num(f1)),\n",
    "                    y=1.1)\n",
    "        # plot labels of true values\n",
    "        if 'true_confusion' in scores[pop_size].keys():\n",
    "            sns.heatmap(scores[pop_size]['true_confusion'], ax=ax[1], square=True,\n",
    "                        xticklabels=False, yticklabels=False)\n",
    "            ax[1].set_title('Labels of true embedding')\n",
    "        else:\n",
    "            ax[1].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = classification_scores['RidgeCV']['var_scores']\n",
    "keys = sorted(classification_scores['RidgeCV']['var_scores'].keys())\n",
    "plot_CV_predict(scores,keys, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_CV_predict(cluster_scores, pop_sizes, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_CV_predict(noisy_scores, sorted(noisy_scores.keys()), 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_distance_recon(reconstructed_distances, orig_distances, size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "if save:\n",
    "    plot_distance_recon(reconstructed_distances, orig_distances, size=15, \n",
    "                       filename=path.join(plot_dir, 'distance_reconstructions.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization of Variability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualizing each factor's reconstruction separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_factor_reconstructions(reconstructions['KNN'], size=15, plot_diagonal=True, plot_regression=False)\n",
    "plot_factor_reconstructions(reconstructions['RidgeCV'], size=15, plot_diagonal=True, plot_regression=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "if save:\n",
    "    plot_factor_reconstructions(reconstructions['KNN'], size=10, plot_diagonal=True, plot_regression=False,\n",
    "                                filename=path.join(plot_dir, 'KNN_factor_reconstructions.png'))\n",
    "    plot_factor_reconstructions(reconstructions['RidgeCV'], size=10, plot_diagonal=True, plot_regression=False,\n",
    "                                filename=path.join(plot_dir, 'RidgeCV_factor_reconstructions.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Using TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More complicate, we can visualize this by looking at the MDS plotting:\n",
    "1. The original DVs\n",
    "2. The \"best\" reconstruction using all the data\n",
    "3. The n_reps simulated estimates with a smaller population size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reconstruction_2D(k_reconstruction, n_reps=30, n_colored=6, use_background=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save:\n",
    "    for name, reconstruction in reconstructions.items():\n",
    "        plot_reconstruction_hist(reconstruction, title='KNN Reconstruction', size=14,\n",
    "                                filename=path.join(plot_dir, name+'_recon_hist.pdf'))\n",
    "        plot_factor_reconstructions(reconstruction, size=10,\n",
    "                                   filename=path.join(plot_dir, name+'_factor_recon.pdf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduced Reconstruction using fewer contextualizing variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
